{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c5ff57",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc1aef2",
   "metadata": {},
   "source": [
    "# Ultrasound imaging simulation\n",
    "\n",
    "Ultrasound imaging is a medical imaging technique that uses sound waves to create visual representations of internal body structures. It is widely used in various medical fields, including obstetrics and cardiology, for diagnostic purposes. The technology relies on the principle that sound waves can penetrate and scatter off tissues, generating echoes that are then used to create detailed images.\n",
    "\n",
    "Ultrasound can be used to measure many different phenomena. Here, we will demonstrate the most common type of ultrasound imaging: B-Mode. B-Mode imaging consists of 3 steps:\n",
    "1. transmit pulse: a transducer emits high-frequency sound waves into the body. These waves are reflected by large tissue boundaries and diffusely scattered by small irregularities (e.g. cell boundaries).\n",
    "2. receive echo: some of these sound waves echo back to the transducer, which records and digitizes them.\n",
    "3. reconstruct image: a \"beamforming\" algorithm converts the pressure time-series into an image of the tissue.\n",
    "\n",
    "Just as we can use NDK to simulate sound waves for focused ultrasound, we can use NDK to simulate the transmitted and received sound waves for ultrasound imaging. Ultrasound imaging simulations allow us to easily manipulate parameters, such as frequency and focal point, and see how they affect the resulting image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7e6b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tqdm.notebook\n",
    "\n",
    "import neurotechdevkit as ndk\n",
    "from neurotechdevkit.sources import PhasedArraySource2D\n",
    "from neurotechdevkit.scenarios.built_in import Scenario3\n",
    "\n",
    "# Imaging modules\n",
    "from neurotechdevkit.imaging import demodulate, beamform, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3361300d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario parameters\n",
    "SPEED_OF_SOUND_WATER = 1500  # meters per second\n",
    "\n",
    "# Plane-wave pulse parameters\n",
    "TONE_CENTER_FREQUENCY = 0.5e6  # Hz\n",
    "TILT_ANGLES_DEG = np.linspace(start=-10, stop=10, endpoint=True, num=21)  # Plane-wave pulses\n",
    "\n",
    "# Phased array transducer parameters\n",
    "ARRAY_PITCH = 300e-6  # meters\n",
    "ARRAY_ELEMENT_WIDTH = 270e-6  # meters\n",
    "ARRAY_NUM_ELEMENTS = 128\n",
    "TRANSDUCER_6DB_PULSE_ECHO_FRACTIONAL_BANDWIDTH = 0.5  # transmit/receive frequency bandwidth as a fraction of center frequency\n",
    "\n",
    "RANDOM_SEED = 58295  # Use if we need the specific function to be deterministic\n",
    "rng = np.random.default_rng(seed=RANDOM_SEED)  # Use if we only need the notebook to be deterministic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2c6428",
   "metadata": {},
   "source": [
    "## Define the scenario\n",
    "\n",
    "Ultrasound imaging measures the scattering of sound waves, which occur due to changes in acoustic impedance (speed and density). In normal tissue, these changes occur across different scales. Large tissue boundaries cause specular scattering, and microscopic heterogeneities cause diffuse scattering. \n",
    "\n",
    "To mimic these properties, we create ultrasound phantoms that are similarly heterogenous across different scales. Their bulk acoustic impedance differs from the background medium (water), and their within-phantom impednances also vary at small scales.\n",
    "\n",
    "For the transducer, we choose a phased array that can both steer ultrasonic waves and record independently at multiple elements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a8c9f1",
   "metadata": {},
   "source": [
    "We visualize the scenario layout with the material masks and then visualize the acoustic properties that affect wave scattering and propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1923a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_scenario(tilt_angle: float = 0.0) -> Scenario3:\n",
    "    \"\"\"Helper function to initialize scenario with different tilt angles.\"\"\"\n",
    "    scenario = Scenario3()\n",
    "    scenario.center_frequency = TONE_CENTER_FREQUENCY\n",
    "    source_position = np.array([0.01, 0.0])\n",
    "    unit_direction = np.array([1.0, 0.0])\n",
    "    # Focus at the depth of the agar phantom\n",
    "    focal_length = np.dot((scenario.target.center - source_position), unit_direction)\n",
    "\n",
    "    scenario.sources = [\n",
    "        PhasedArraySource2D(\n",
    "            position=source_position,\n",
    "            direction=unit_direction,\n",
    "            num_elements=ARRAY_NUM_ELEMENTS,\n",
    "            num_points=ARRAY_NUM_ELEMENTS * 4,\n",
    "            tilt_angle=tilt_angle,\n",
    "            focal_length=focal_length,\n",
    "            pitch=ARRAY_PITCH,\n",
    "            element_width=ARRAY_ELEMENT_WIDTH,\n",
    "        )\n",
    "    ]\n",
    "    scenario.make_grid()\n",
    "    scenario.compile_problem()\n",
    "    return scenario\n",
    "\n",
    "scenario = create_scenario()\n",
    "scenario.render_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2012c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the relevant material metrics\n",
    "fig,axs = plt.subplots(1,3,figsize=(15,5))\n",
    "\n",
    "for idx, attribute in enumerate([\"vp\", \"rho\", \"alpha\"]):\n",
    "    im = axs[idx].imshow(getattr(scenario.problem.medium, attribute).data)\n",
    "    plt.colorbar(im, ax=axs[idx])\n",
    "    axs[idx].set_title(attribute)\n",
    "    axs[idx].set_xlabel(\"y [a.u.]\")\n",
    "    axs[idx].set_ylabel(\"x [a.u.]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58855e13",
   "metadata": {},
   "source": [
    "We can see the bulk contrast between the background medium and the phantoms.\n",
    "\n",
    "Speed-of-sound (`vp`) and density (`rho`) determine a material's \"acoustic impedance.\" Changes in acoustic impedance determine how ultrasound waves scatter, while attenuation (`alpha`) determines how far ultrasound waves into a medium."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a83d64",
   "metadata": {},
   "source": [
    "## 1. Transmit pulse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b903b6e7",
   "metadata": {},
   "source": [
    "### Visualize beam\n",
    "\n",
    "To create a 2-D ultrasound image, a common approach is to emit a focused beam at a single horizontal position, effectively creating a 1-D depth image, and then repeat at different horizontal positions. We take this approach here, but for simplicity only simulate a single pulse.\n",
    "\n",
    "Let's verify that the transducer focuses along a single line. This will be the line that receives the most ultrasonic energy and thus will reflect the strongest echoes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda24f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = create_scenario().simulate_steady_state()\n",
    "result.render_steady_state_amplitudes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac808c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run pulse for imaging\n",
    "scenario = create_scenario()\n",
    "def calc_simulation_time(scenario: Scenario3):\n",
    "    simulation_time = ndk.scenarios._time.select_simulation_time_for_pulsed(\n",
    "        grid=scenario.grid,\n",
    "        materials=scenario.materials,\n",
    "        delay=ndk.scenarios._time.find_largest_delay_in_sources(scenario.sources),\n",
    "    )\n",
    "    # Run pulse for twice the standard simulation time to allow for reflections\n",
    "    return 2 * simulation_time\n",
    "\n",
    "result = scenario.simulate_pulse(simulation_time=calc_simulation_time(scenario))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c437a4",
   "metadata": {},
   "source": [
    "## 2. Receive echo\n",
    "(This is actually already included in `scenario.simulate_pulse`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22de77f5",
   "metadata": {},
   "source": [
    "## Visualizing received (simulated) signals\n",
    "\n",
    "While NDK does provide us the entire wavefield, in a real imaging situation, we usually only have access to the \"radiofrequency signals,\" the raw signals measured at the ultrasound sensor elements. Let's visualize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5578aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The receivers at N sensor elements give us data traces of [N, num_fast_time_samples]\n",
    "num_channels = result.shot.num_receivers\n",
    "assert num_channels > 0\n",
    "time_arr = result.traces.time.grid\n",
    "assert result.traces.data.shape == (\n",
    "    num_channels,\n",
    "    len(time_arr)\n",
    ")\n",
    "\n",
    "print(\"Traces shape:\", result.traces.data.shape)\n",
    "print(\"Time grid:\", time_arr)\n",
    "freq_sampling = 1 / result.traces.time.step\n",
    "print(\"Sampling frequency [Hz]: {:.2e}\".format(freq_sampling))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4b3878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape to [time, channels] shape\n",
    "rf_signals = result.traces.data.T  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51002b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_VISUALIZE = 8\n",
    "channel_idxs = rng.integers(num_channels, size=NUM_VISUALIZE)\n",
    "channel_idxs.sort()\n",
    "\n",
    "time_mask = (40e-6 < time_arr) & (time_arr < 140e-6)\n",
    "# time_mask = time_arr > 0\n",
    "\n",
    "# Plot with some offsets\n",
    "_ = plt.plot(\n",
    "    time_arr[time_mask] * 1000,\n",
    "    rf_signals[time_mask][:, channel_idxs] + np.linspace(-1000, 1000, num=NUM_VISUALIZE, endpoint=True),\n",
    "    label=[f\"channel-{channel_idx}\" for channel_idx in channel_idxs],\n",
    ")\n",
    "_ = plt.legend()\n",
    "_ = plt.title(\"Example radiofrequency signals (RF)\")\n",
    "_ = plt.xlabel(\"time [ms]\")\n",
    "_ = plt.ylabel(\"amplitude and channel offset [a.u]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa858f8",
   "metadata": {},
   "source": [
    "The transmitted signals consisted of a high-frequency pulse, so the received RF signals are similarly modulated by the same carrier frequency. The signals are strongest about 6-9ms after the pulse transmission.\n",
    "\n",
    "The received amplitudes here scale with the transmit pulse, which currently has an arbitrary amplitude. Later down, we rescale the image anyways, so we won't worry about absolute amplitudes here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bc3ee4",
   "metadata": {},
   "source": [
    "## 3. Reconstruct image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944bb0f9",
   "metadata": {},
   "source": [
    "### 3a. Demodulate radiofrequency (RF) signals to in-phase/quadrature (I/Q)\n",
    "\n",
    "Next, we extract the lower-frequency envelope from the radiofrequency signals. One can think of this lower-frequency envelope as matching the spatial frequency of the image.\n",
    "\n",
    "There are two slight variants of this initial signal processing step: quadrature detection (I/Q) or analytic (Hilbert transform). I/Q is traditionally more common in real-time systems, so we will show it here. https://starfishmedical.com/blog/pros-cons-two-popular-ultrasound-signal-processing-techniques/ provides a nice comparison of the two methods.\n",
    "\n",
    "I/Q demodulates the center/carrier frequency from the RF signal to extract the lower-frequency envelope as a complex signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9835d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "iq_signals, freq_carrier_est = demodulate.demodulate_rf_to_iq(\n",
    "    rf_signals,\n",
    "    freq_sampling,\n",
    "    freq_carrier=TONE_CENTER_FREQUENCY,\n",
    "    bandwidth=TRANSDUCER_6DB_PULSE_ECHO_FRACTIONAL_BANDWIDTH,\n",
    ")\n",
    "\n",
    "assert iq_signals.shape == rf_signals.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39420fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the lower-frequency I/Q signals over the RF signal\n",
    "channel_idx = rng.integers(num_channels)  # Pick one of the earlier channels? random choice?\n",
    "# zoom in on time-window\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(\n",
    "    time_arr[time_mask] * 1000,\n",
    "    rf_signals[time_mask, channel_idx],\n",
    "    color='k',\n",
    "    alpha=0.3,\n",
    "    label='RF',\n",
    ")\n",
    "ax.plot(\n",
    "    time_arr[time_mask] * 1000,\n",
    "    iq_signals[time_mask, channel_idx].real,\n",
    "    label='in-phase (I)'\n",
    ")\n",
    "ax.plot(\n",
    "    time_arr[time_mask] * 1000,\n",
    "    iq_signals[time_mask, channel_idx].imag,\n",
    "    label='quadrature (Q)'\n",
    ")\n",
    "ax.legend()\n",
    "ax.set_title(f\"RF & I/Q signal. Channel: {channel_idx}\")\n",
    "ax.set_xlabel(\"time [ms]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b3476a",
   "metadata": {},
   "source": [
    "### 3b. Beamform image\n",
    "Let's use a common beamforming algorithm, `delay-and-sum`, to reconstruct an ultrasound image.\n",
    "\n",
    "The intuition for `delay-and-sum` is that: if an object is at distance $d$ from a given transducer with speed-of-sound $c$, we should receive its scattered signal at time $\\frac{d}{c}$ after it receives the ultrasound pulse. This is a similar idea to shouting in a cave, then waiting to hear the echo-time to determine how far away the wall is. In 2 or 3 dimensions, however, there are multiple points at the same distance that could be contributing to the echo signal. Delay-and-sum implicitly disassociates these confounded points by summing over the different receiver elements.\n",
    "\n",
    "Note: beamforming can also work on the raw radiofrequency (RF) signals. However, the RF signals are higher-frequency and thus require additional computational power, so diagnostic B-mode scanners usually demodulate RF signals to I/Q first (as we did above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664cbb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beam-form I/Q signals into an image\n",
    "\n",
    "assert len(scenario.sources) == 1\n",
    "source = scenario.sources[0]\n",
    "assert isinstance(source, ndk.sources.PhasedArraySource)\n",
    "pitch = source.pitch\n",
    "width = source.element_width\n",
    "empirical_pitch = np.linalg.norm(np.diff(result.shot.receiver_coordinates, axis=0), axis=1)\n",
    "np.testing.assert_allclose(empirical_pitch, pitch, rtol=1e-2)\n",
    "\n",
    "# Generate an image at the scenario grid\n",
    "# NOTE: .mesh uses different x/y space\n",
    "x_mesh, y_mesh = result.shot.grid.space.mesh\n",
    "\n",
    "# Switch to imaging convention: x for parallel-to-array, z for depth\n",
    "imaging_x_mesh = y_mesh + scenario.origin[1]\n",
    "imaging_z_mesh = x_mesh + scenario.origin[0]\n",
    "\n",
    "iq_signals_beamformed = beamform.beamform_delay_and_sum(\n",
    "    iq_signals,\n",
    "    x=imaging_x_mesh,\n",
    "    z=imaging_z_mesh,\n",
    "    pitch=pitch,\n",
    "    tx_delays=scenario.sources[0].element_delays,\n",
    "    freq_sampling=freq_sampling,\n",
    "    freq_carrier=TONE_CENTER_FREQUENCY,\n",
    "    f_number=None,\n",
    "    width=width,\n",
    "    bandwidth=TRANSDUCER_6DB_PULSE_ECHO_FRACTIONAL_BANDWIDTH,\n",
    "    speed_sound=SPEED_OF_SOUND_WATER,  # water\n",
    ")\n",
    "iq_signals_beamformed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b37712d",
   "metadata": {},
   "source": [
    "## Visualize reconstructed image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bdea02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ultrasound_image(x_mesh, z_mesh, iq_signals_bf, db=40):\n",
    "    plt.pcolormesh(\n",
    "        x_mesh,\n",
    "        z_mesh,\n",
    "        util.log_compress(iq_signals_bf, db),\n",
    "        cmap='gray',\n",
    "    )\n",
    "    cbar = plt.colorbar(ticks=[0, 1])\n",
    "    cbar.ax.set_yticklabels([f\"-{db} dB\", \"0 dB\"])  # horizontal colorbar\n",
    "\n",
    "    plt.axis('equal')\n",
    "    plt.gca().invert_yaxis()  # Invert the y-axis to flip the image vertically\n",
    "    plt.title('Log-compressed image')\n",
    "    plt.xlabel('[m]')\n",
    "    plt.ylabel('[m]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c61f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore the transducer elements\n",
    "mask = (imaging_z_mesh > 0.02).all(axis=1)\n",
    "plot_ultrasound_image(\n",
    "    imaging_x_mesh[mask],\n",
    "    imaging_z_mesh[mask],\n",
    "    iq_signals_beamformed[mask],\n",
    "    db=30,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e46a7c",
   "metadata": {},
   "source": [
    "Above shows the reconstruction of a single ultrasound scan line, focusing on $x = 0$ (e.g., see the steady-state amplitude, which shows where the pulse focuses). Because the phantom on the right is out of the focal point, it does not show up clearly in the image. Sweeping across different scan lines would allow reconstruction of a full 2D (or 3D) image.\n",
    "\n",
    "The speckles in the image are a normal part of B-mode imaging. Further signal processing methods, such as harmonic imaging or compound-plane-wave imaging, can improve spatial resolution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8633582e",
   "metadata": {},
   "source": [
    "## Construct full image by sweeping scan-lines\n",
    "\n",
    "Next, let's build on the previous example by constructing a wider field-of-view by sweeping different scan lines. We can use the phased-array to tilt the scan line radially and image different parts of the image space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a354a874",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TILTS = 5\n",
    "TILT_ANGLES_DEG = np.linspace(-40, 40, NUM_TILTS, endpoint=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2061a42",
   "metadata": {},
   "source": [
    "Let's compare the beam-focus of 2 different tilt angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce78fccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario = create_scenario(tilt_angle=TILT_ANGLES_DEG[0])\n",
    "assert len(scenario.sources) == 1\n",
    "result = scenario.simulate_steady_state()\n",
    "result.render_steady_state_amplitudes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3f7522",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario = create_scenario(tilt_angle=TILT_ANGLES_DEG[-1])\n",
    "assert len(scenario.sources) == 1\n",
    "result = scenario.simulate_steady_state()\n",
    "result.render_steady_state_amplitudes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233fa85e",
   "metadata": {},
   "source": [
    "The steady-state figures show that the tilted ultrasound beams hit different parts of the image space.\n",
    "\n",
    "Now, let's pulse at different scan lines and sum them together to reconstruct the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b88a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send several plane waves from the transducer and measure the reflected/scattered acoustic waves\n",
    "# We can then compound the different plane waves to improve the spatial resolution\n",
    "\n",
    "results = [None] * len(TILT_ANGLES_DEG)\n",
    "# keep track of the tx delays used\n",
    "element_delays_list = [None] * len(TILT_ANGLES_DEG)\n",
    "\n",
    "for idx, tilt_angle in enumerate(tqdm.notebook.tqdm(TILT_ANGLES_DEG, desc=\"Simulating pulses\", unit=\"pulse\")):\n",
    "    # Current limitation of NDK: need to re-generate scenario to simulate a new pulse\n",
    "    # https://github.com/agencyenterprise/neurotechdevkit/issues/108\n",
    "    # Set the tilt angle, which will automatically re-calculate the element delays\n",
    "    scenario = create_scenario(tilt_angle=tilt_angle)\n",
    "    assert len(scenario.sources) == 1\n",
    "    element_delays_list[idx] = scenario.sources[0].element_delays\n",
    "    results[idx] = scenario.simulate_pulse(simulation_time=calc_simulation_time(scenario))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f50f684",
   "metadata": {},
   "source": [
    "Let's follow the same steps as above for reconstructing each scan-line image.\n",
    "\n",
    "Stack the different scan-lines (sometimes called the \"slow-time\" dimension)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8c0f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad RF signals to longest length, in case they are not all the same length\n",
    "rf_signal_lengths = [len(result.traces.time.grid) for result in results]\n",
    "rf_signal_max_len = max(rf_signal_lengths)\n",
    "rf_signal_max_len_idx = np.argmax(rf_signal_lengths)\n",
    "time = results[rf_signal_max_len_idx].traces.time.grid\n",
    "\n",
    "# Shape: [num_fast_time_samples, num_elements, num_pulses]\n",
    "rf_signals = np.zeros(\n",
    "    (rf_signal_max_len, ARRAY_NUM_ELEMENTS, len(results)),\n",
    "    dtype=float,\n",
    ")\n",
    "for pulse_idx, result in enumerate(results):\n",
    "    rf_signals[:rf_signal_lengths[pulse_idx], :, pulse_idx] = result.traces.data.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b008760",
   "metadata": {},
   "source": [
    "Demodualte the RF signals to I/Q."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca42e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_sampling = 1 / result.traces.time.step\n",
    "iq_signals, _ = demodulate.demodulate_rf_to_iq(\n",
    "    rf_signals,\n",
    "    freq_sampling,\n",
    "    freq_carrier=TONE_CENTER_FREQUENCY,\n",
    "    bandwidth=TRANSDUCER_6DB_PULSE_ECHO_FRACTIONAL_BANDWIDTH,\n",
    ")\n",
    "\n",
    "assert iq_signals.shape == rf_signals.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b306d4",
   "metadata": {},
   "source": [
    "Beamform I/Q signals into an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6169aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beam-form I/Q signals into an image\n",
    "\n",
    "assert len(scenario.sources) == 1\n",
    "source = scenario.sources[0]\n",
    "assert isinstance(source, ndk.sources.PhasedArraySource)\n",
    "pitch = source.pitch\n",
    "width = source.element_width\n",
    "empirical_pitch = np.linalg.norm(np.diff(results[0].shot.receiver_coordinates, axis=0), axis=1)\n",
    "np.testing.assert_allclose(empirical_pitch, pitch, rtol=1e-2)\n",
    "\n",
    "# Generate an image at the scenario grid\n",
    "# NOTE: .mesh uses different x/y space\n",
    "x_mesh, y_mesh = results[0].shot.grid.space.mesh\n",
    "\n",
    "# Switch to imaging convention: x for parallel-to-array, z for depth\n",
    "imaging_x_mesh = y_mesh + scenario.origin[1]\n",
    "imaging_z_mesh = x_mesh + scenario.origin[0]\n",
    "\n",
    "iq_signals_beamformed_list = []\n",
    "for idx, tilt_angle in enumerate(TILT_ANGLES_DEG):\n",
    "    iq_signals_beamformed = beamform.beamform_delay_and_sum(\n",
    "        iq_signals[:, :, idx],\n",
    "        x=imaging_x_mesh,\n",
    "        z=imaging_z_mesh,\n",
    "        pitch=pitch,\n",
    "        tx_delays=element_delays_list[idx],\n",
    "        freq_sampling=freq_sampling,\n",
    "        freq_carrier=TONE_CENTER_FREQUENCY,\n",
    "        f_number=None,\n",
    "        width=width,\n",
    "        bandwidth=TRANSDUCER_6DB_PULSE_ECHO_FRACTIONAL_BANDWIDTH,\n",
    "        speed_sound=SPEED_OF_SOUND_WATER,  # water\n",
    "    )\n",
    "    iq_signals_beamformed_list.append(iq_signals_beamformed)\n",
    "\n",
    "iq_signals_beamformed_compound = np.stack(iq_signals_beamformed_list, axis=-1)\n",
    "iq_signals_beamformed_compound.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a14737",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (imaging_z_mesh > 0.02).all(axis=1)\n",
    "plot_ultrasound_image(\n",
    "    imaging_x_mesh[mask],\n",
    "    imaging_z_mesh[mask],\n",
    "    iq_signals_beamformed_compound[mask].sum(axis=-1),\n",
    "    db=30,\n",
    ")\n",
    "plt.title(\"Multi-scan-line image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f054533",
   "metadata": {},
   "source": [
    "The multi-scan-line image includes both phantoms. Notice, though, that the image of the center phantom is more intense where the scan line was focused. This resolution can be improved by sweeping scan lines that are closer together. Thus, the resolution is tightly coupled with the number of scan lines, analogous to raster-printing a photo.\n",
    "\n",
    "In the next notebook, we will discuss a method to improve resolution with a fewer number of pulse-echos by using unfocused beams."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
